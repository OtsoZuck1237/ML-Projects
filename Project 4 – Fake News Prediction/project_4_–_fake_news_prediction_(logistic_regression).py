# -*- coding: utf-8 -*-
"""Project 4 â€“ Fake News Prediction (Logistic Regression).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GTa2hkW-aRrnair_H9oaAoX-a358JUBG

#Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re #search mechanism that is used in defining the stemming function
from nltk.corpus import stopwords #useless words that can be reduced in texts
from nltk.stem.porter import PorterStemmer #used for stemming
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score

import nltk
nltk.download('stopwords')

print(stopwords.words('english'))

"""#Data Preprocessing"""

df = pd.read_csv('train.csv')
X = df.iloc[:, :-1].values
Y = df.iloc[:, -1].values

#how many rows and columns?
df.shape

#printing 5x5 set
df.head()

#counting the number of missing values in our dataset
df.isnull().sum()

#let's fill our missing values with null string meaning just ''
df = df.fillna('')

#now we are combining our author column with title column into the new column called 'content'
df['content'] = df['author']+' '+df['title']

print(df['content'])

"""Now we do something called Stemming

Stemming is basically the process of reducing the words into their roots. For example, the words like actor, actress, acting --> can be all reduced to the root word 'act'. Why we do it? Simply because we need to get rid of the as much words as possible to enhance our perfomance
"""

port_stem = PorterStemmer()

def stemming(content):
  stemmed_content = re.sub('[^a-zA-Z]',' ',content) #this removes everything in text, except the alphabet letters
  stemmed_content = stemmed_content.lower() #we want to convert all content words into the lowercase, because learning model can confuse himself, like uppecase is more important or smth like that, just avoiding some confusion to our model
  stemmed_content = stemmed_content.split()
  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')] #we just taking the words that are NOT the stopwords
  stemmed_content = ' '.join(stemmed_content) #just joining all words together
  return stemmed_content

df['content'] = df['content'].apply(stemming)

print(df['content'])

X = df['content'].values
Y = df['label'].values

print(X)

print(Y)

"""Now our data is almost ready, now we just need to convert our textual data into the numerical, so that our machine understands it better"""

vectorizer = TfidfVectorizer()
X=vectorizer.fit_transform(X)

print(X)

"""#Splitting the dataset into the Training set and Test set"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 2)

"""#Training the Logistic Regression model on the Training set"""

classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, Y_train)

"""#Making the Confusion matrix"""

Y_pred = classifier.predict(X_test)
cm = confusion_matrix(Y_test, Y_pred)
print(cm)
accuracy_score(Y_test, Y_pred)

"""#Now let's make some prediction"""

X_new = X_test[0]
prediction = classifier.predict(X_new)
print(prediction)

if (prediction==0):
  print('The news is Real')
else:
  print('The news is Fake')

# prompt: Make some prediction

# Making a new prediction
X_new = X_test[3] # Predicting for the 4th item in the test set
prediction = classifier.predict(X_new)
print(prediction)

if (prediction[0] == 0):  # Access the prediction value (it's an array)
  print('The news is Real')
else:
  print('The news is Fake')